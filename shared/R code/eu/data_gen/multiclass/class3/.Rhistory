# 1e-323==0
## Algorithm2.
### Create Q matrix
Q <- matrix(0,K,K)
for(i in 1:K){
for(j in 1:K){
Q[i,j] <- -r[j,i]*r[i,j]
}
}
diag(Q) <- colSums(r^2)
### (1) Initialize P
p <- matrix(rep(1/K),K)
p[K] <- 1-sum(p[-K])
### (2) Repeat (tt = 1, 2, 3, ..., K, 1, ...)
tt <- 1
iter.n <- 1
while(iter.n <= max.iter){
a <- 1/Q[tt,tt]
b <- t(p) %*% Q %*% p
p[tt,] <- a * ( -as.vector(Q[tt,-tt]) %*% p[-tt]  + b)
## normalize
p <- p/sum(p)
## Condition (21) check
tmp <- Q %*% p
tmp2 <- matrix(outer(tmp, tmp, "-"), K, K)
tmp3 <- tmp2[upper.tri(tmp2)]
# idx <- which(max(p) == p)
# c <- c(p[idx] - p[-idx])
if (abs(tmp3)[1] < 1e-05 && abs(tmp3)[2] < 1e-05 && abs(tmp3)[3] < 1e-05) break
# if (abs(tmp3) < 1e-05 && sum(p) == 1) break
# if (length(unique(sign(tmp))) == 1 && abs(tmp3) < 1e-05 && sum(p) == 1) break
## re-indexing t
if (tt == K) {
tt <- 1
} else {
tt <- (tt + 1)
}
iter.n <- iter.n + 1 # counting the iteration
}
if (iter.n == max.iter) warning("maximum iteration reached!")
pi.fl.one.simul[ii,] <- p
} #--- End of n.test loop ------------------------------------------------------------------------#
# 시뮬레이션 1번=test sample은 n.test개. 따라서 p도 시뮬 한번 당 n.test개 나옴 -------------------#
# ------------------------------ Simulation on FSVM ----------------------------------------------#
####=======================     train data      ===============================####
# calculate the pi path
obj12 <- tryCatch(fsvm.prob(train.x.12, train.y.12, t, L), error = function(e) fsvm.prob(train.x.12, train.y.12, t, L, ridge=1e-3))
obj13 <- tryCatch(fsvm.prob(train.x.13, train.y.13, t, L), error = function(e) fsvm.prob(train.x.13, train.y.13, t, L, ridge=1e-3))
obj23 <- tryCatch(fsvm.prob(train.x.23, train.y.23, t, L), error = function(e) fsvm.prob(train.x.23, train.y.23, t, L, ridge=1e-3))
# obj12 <- fsvm.prob(train.x.12, train.y.12, t, L)
# obj13 <- fsvm.prob(train.x.13, train.y.13, t, L)
# obj23 <- fsvm.prob(train.x.23, train.y.23, t, L)
####======================= predict probability ===============================####
obj212 <- predict.fsvm.prob(obj12, test.x)
obj213 <- predict.fsvm.prob(obj13, test.x)
obj223 <- predict.fsvm.prob(obj23, test.x)
# 시뮬레이션 1번=test sample은 n.test개. 따라서 p도 시뮬 한번 당 n.test개 나옴 -------------------#
# 시뮬당 저장공간 생성
pi.svm.one.simul <- matrix(0, n.test, K)
# Pairwise Coupling
for(ii in 1:n.test){
r <- matrix(0, K, K)
r[lower.tri(r)] <- c(obj212$prob[ii], # r21
obj213$prob[ii], # r31
obj223$prob[ii]) # r32
r[upper.tri(r)] <- t(r)[upper.tri(r)]
one <- matrix(1,K,K)
one[lower.tri(one, diag=TRUE)] <- 0
r <- one - r
r <- abs(r)
## Algorithm2.
### Create Q matrix
Q <- matrix(0,K,K)
for(i in 1:K){
for(j in 1:K){
Q[i,j] <- -r[j,i]*r[i,j]
}
}
diag(Q) <- colSums(r^2)
### (1) Initialize P
p <- matrix(rep(1/K),K)
p[K] <- 1-sum(p[-K])
### (2) Repeat (tt = 1, 2, 3, ..., K, 1, ...)
tt <- 1
iter.n <- 1
while(iter.n <= max.iter){
a <- 1/Q[tt,tt]
b <- t(p) %*% Q %*% p
p[tt,] <- a * ( -as.vector(Q[tt,-tt]) %*% p[-tt]  + b)
## normalize
p <- p/sum(p)
## Condition (21) check
tmp <- Q %*% p
tmp2 <- matrix(outer(tmp, tmp, "-"), K, K)
tmp3 <- tmp2[upper.tri(tmp2)]
# idx <- which(max(p) == p)
# c <- c(p[idx] - p[-idx])
if (abs(tmp3)[1] < 1e-05 && abs(tmp3)[2] < 1e-05 && abs(tmp3)[3] < 1e-05) break
# if (abs(tmp3) < 1e-05 && sum(p) == 1) break
# if (length(unique(sign(tmp))) == 1 && abs(tmp3) < 1e-05 && sum(p) == 1) break
## re-indexing t
if (tt == K) {
tt <- 1
} else {
tt <- (tt + 1)
}
iter.n <- iter.n + 1 # counting the iteration
}
if (iter.n == max.iter) warning("maximum iteration reached!")
pi.svm.one.simul[ii,] <- p
} #--- End of n.test loop ------------------------------------------------------------------------#
# 시뮬레이션 1번=test sample은 n.test개. 따라서 p도 시뮬 한번 당 n.test개 나옴 -------------------#
# Save results (for flogistic) -------------------------------------------------------------------#
# CRE
s.fl <- c()
for (k in 1:n.test){
s.fl <- c(s.fl,-log(pi.fl.one.simul[k,test.y[k]]))
}
CRE.fl <- sum(s.fl)
CRE.fl.result[iter] <- CRE.fl
# pi.star
pi.fl.result[[iter]] <- pi.fl.one.simul
# Save results (for FSVM) -----------------------------------------------------------------------#
# CRE
s.svm <- c()
for (k in 1:n.test){
s.svm <- c(s.svm,-log(pi.svm.one.simul[k,test.y[k]]))
}
CRE.svm <- sum(s.svm)
CRE.svm.result[iter] <- CRE.svm
# Answer
ans[[iter]] <- test.y
ans.p[[iter]] <- true.p
# pi.star
pi.svm.result[[iter]] <- pi.svm.one.simul
}
# Check warnings
summary(warnings())
# Critereon (1) Cross Entropy
# CRE.fl.result[1]
# CRE.svm.result[1]
fl.cre<-round(mean(CRE.fl.result),digits = 3)
svm.cre<-round(mean(CRE.svm.result),digits = 3)
# Critereon (2) Accuracy
svm <- matrix(0, ncol = n.sim)
flog <- matrix(0, ncol = n.sim)
for(k in 1:n.sim){
flog[,k] <- sum(apply(pi.fl.result[[k]], 1, which.max) == ans[[k]])
svm[,k] <- sum(apply(pi.svm.result[[k]], 1, which.max) == ans[[k]])
}
# total number : 50 * 30 = 1500
svm.acc <- round(mean(svm/n.test), digits = 3)
fl.acc <- round(mean(flog/n.test), digits = 3)
# Critereon (3) Distance btw true p & hat p
predict.p.svm <- as.list(1:n.sim)
predict.p.fl <- as.list(1:n.sim)
for(i in 1:n.sim){
idx <- ans[[i]]
for(j in 1:n.test){
a <- pi.svm.result[[i]][j,]
b <- pi.fl.result[[i]][j,]
predict.p.svm[[i]][j] <- a[idx[j]]
predict.p.fl[[i]][j] <- b[idx[j]]
}
}
# calculate the difference
diff.svm <- rep(0,n.sim)
diff.fl <- rep(0,n.sim)
# w.diff.svm <- rep(0,n.sim)
# w.diff.fl <- rep(0,n.sim)
for (i in 1:n.sim){
diff.svm[i] <- mean(abs(ans.p[[i]] - predict.p.svm[[i]]))
diff.fl[i] <- mean(abs(ans.p[[i]] - predict.p.fl[[i]]))
}
# print
paste0("--------------Simulation Result cov=", cov," Error =",error,"--------------")
paste0("--------------Criterieon (1) Accuracy --------------")
paste0("Accuracy of FSVM is ", round(svm.acc, digits = 3))
paste0("Accuracy of Flogistic is ", round(fl.acc, digits = 3))
paste0("--------------Criterieon (2) p diff --------------")
paste0("mean(Diffence) btw true p and svm.predicted p is ", round(mean(diff.svm), digits = 3))
paste0("mean(Diffence) btw true p and fl.predicted p is ", round(mean(diff.fl), digits = 3))
##########################################
## PLOT Gaussian Process data (class 3) ##
##########################################
rm(list = ls())
#### load packages & R code ####
library(mvtnorm);library(fda)
setwd('C:/Users/eugene/Desktop/SVM/shared/R code/eu/data_gen/multiclass/class3')
source('linear.cross.3.R')
source('linear.par.3.R')
source('nonlinear.cross.3.R')
source('nonlinear.par.3.R')
# Set Parameters
n <- 30; error <- 0.3; t <- seq(0, 1, by = 0.05); beta <- 1
cov="I";rho=1
# Generate data
# data <- linear.cross.3(n, error, cov, rho, t, seed = 1)
# data <- linear.par.K(n, error, beta, K, cov, rho, t, seed = 1)
data <- nonlinear.cross.3(n, error, cov, rho, t, seed = 1)
# index for 1, 2, 3, 4 class
idx.1 <- which(data$y == 1)
idx.2 <- which(data$y == 2)
idx.3 <- which(data$y == 3)
# Check max value and plot the background
a <- c()
for (i in 1:n) a <- c(a, unlist(data$x[[i]]))
max(a)
plot(t, data$x[[1]], ylim = c(min(a),max(a)))
for (i in idx.1) lines(t, data$x[[i]], col = 1)
for (i in idx.2) lines(t, data$x[[i]], col = 2)
for (i in idx.3) lines(t, data$x[[i]], col = 3)
set.seed(seed)
seed = 1
set.seed(seed)
K <- 3
# Check the Division part
if (n %% K != 0) message(n, " is not divisible by ",K)
##== Class labeling at y ==##
y <- rep(1,n)
quotient <- n %/% K
a <- c(1:n)
num <- seq(0, n, quotient)
label <- c(1:K)
for (i in 1:K){
y[(num[i]+1):num[i+1]] <- label[i]
}
# spatio temporal predictor
n.t <- length(t)
# covariance matrix of gaussian process
if (cov=="I") {
Sigma <- error*diag(n.t)
} else if(cov=="AR") {
rho <- rho # 0.7
idx.t <- c(1:n.t)
Sigma <- error * outer(idx.t, idx.t, function(a, b) rho^abs(a-b))
} else if(cov=="CS") {
rho <- rho # 0.3
tmp <- matrix(rho,n.t,n.t)
diag(tmp) <- 1
Sigma <- error * tmp
} else warning("covariance structure was not specified")
# empty x.list
x.list <- as.list(1:n)
idx2 <- which(y==2)
# Divide index
idx1 <- which(y==1)
idx3 <- which(y==3)
# Create mean vector
mu.t1 <- sin(3*t)
mu.t2 <- -sin(3*t)
mu.t3 <- sin(0.1*t)
# x.list
x.list <- as.list(1:n)
for (i in 1:n) {
if(i %in% idx1) x.list[[i]] <- rmvnorm(1, mu.t1, Sigma)
else if(i %in% idx2) x.list[[i]] <- rmvnorm(1, mu.t2, Sigma)
else if(i %in% idx3) x.list[[i]] <- rmvnorm(1, mu.t3, Sigma)
}
# Calculate the True p
true.p <- rep(0,n)
for(i in 1:n){
a1 <- dmvnorm(x=x.list[[i]], mean = mu.t1,log=TRUE)
a2 <- dmvnorm(x=x.list[[i]], mean = mu.t2,log=TRUE)
a3 <- dmvnorm(x=x.list[[i]], mean = mu.t3,log=TRUE)
total <- exp(a1)+exp(a2)+exp(a3)
if (i %in% idx1) true.p[i] <- exp(a1)/total
else if (i %in% idx2) true.p[i] <- exp(a2)/total
else if (i %in% idx3) true.p[i] <- exp(a3)/total
else warning(paste0("No class was assigned for obs ",i))
}
true.p
n<-1000
set.seed(seed)
K <- 3
# Check the Division part
if (n %% K != 0) message(n, " is not divisible by ",K)
##== Class labeling at y ==##
y <- rep(1,n)
quotient <- n %/% K
a <- c(1:n)
num <- seq(0, n, quotient)
label <- c(1:K)
for (i in 1:K){
y[(num[i]+1):num[i+1]] <- label[i]
}
##== Labeling End ==##
# spatio temporal predictor
n.t <- length(t)
# covariance matrix of gaussian process
if (cov=="I") {
Sigma <- error*diag(n.t)
} else if(cov=="AR") {
rho <- rho # 0.7
idx.t <- c(1:n.t)
Sigma <- error * outer(idx.t, idx.t, function(a, b) rho^abs(a-b))
} else if(cov=="CS") {
rho <- rho # 0.3
tmp <- matrix(rho,n.t,n.t)
diag(tmp) <- 1
Sigma <- error * tmp
} else warning("covariance structure was not specified")
# empty x.list
x.list <- as.list(1:n)
# Divide index
idx1 <- which(y==1)
idx2 <- which(y==2)
idx3 <- which(y==3)
# Create mean vector
mu.t1 <- sin(3*t)
mu.t2 <- -sin(3*t)
mu.t3 <- sin(0.1*t)
# x.list
x.list <- as.list(1:n)
for (i in 1:n) {
if(i %in% idx1) x.list[[i]] <- rmvnorm(1, mu.t1, Sigma)
else if(i %in% idx2) x.list[[i]] <- rmvnorm(1, mu.t2, Sigma)
else if(i %in% idx3) x.list[[i]] <- rmvnorm(1, mu.t3, Sigma)
}
# Calculate the True p
true.p <- rep(0,n)
for(i in 1:n){
a1 <- dmvnorm(x=x.list[[i]], mean = mu.t1,log=TRUE)
a2 <- dmvnorm(x=x.list[[i]], mean = mu.t2,log=TRUE)
a3 <- dmvnorm(x=x.list[[i]], mean = mu.t3,log=TRUE)
total <- exp(a1)+exp(a2)+exp(a3)
if (i %in% idx1) true.p[i] <- exp(a1)/total
else if (i %in% idx2) true.p[i] <- exp(a2)/total
else if (i %in% idx3) true.p[i] <- exp(a3)/total
else warning(paste0("No class was assigned for obs ",i))
}
true.p
sum(true.p <0.9)
set.seed(seed)
K <- 3
# Check the Division part
if (n %% K != 0) message(n, " is not divisible by ",K)
##== Class labeling at y ==##
y <- rep(1,n)
quotient <- n %/% K
a <- c(1:n)
num <- seq(0, n, quotient)
label <- c(1:K)
for (i in 1:K){
y[(num[i]+1):num[i+1]] <- label[i]
}
##== Labeling End ==##
# spatio temporal predictor
n.t <- length(t)
# covariance matrix of gaussian process
if (cov=="I") {
Sigma <- error*diag(n.t)
} else if(cov=="AR") {
rho <- rho # 0.7
idx.t <- c(1:n.t)
Sigma <- error * outer(idx.t, idx.t, function(a, b) rho^abs(a-b))
} else if(cov=="CS") {
rho <- rho # 0.3
tmp <- matrix(rho,n.t,n.t)
diag(tmp) <- 1
Sigma <- error * tmp
} else warning("covariance structure was not specified")
# empty x.list
x.list <- as.list(1:n)
# Divide index
idx1 <- which(y==1)
idx2 <- which(y==2)
idx3 <- which(y==3)
# Create mean vector
mu.t1 <- 2*t
mu.t2 <- -2*t
mu.t3 <- rep(0.5,n.t)
# x.list
x.list <- as.list(1:n)
for (i in 1:n) {
if(i %in% idx1) x.list[[i]] <- rmvnorm(1, mu.t1, Sigma)
else if(i %in% idx2) x.list[[i]] <- rmvnorm(1, mu.t2, Sigma)
else if(i %in% idx3) x.list[[i]] <- rmvnorm(1, mu.t3, Sigma)
}
# Calculate the True p
true.p <- rep(0,n)
for(i in 1:n){
a1 <- dmvnorm(x=x.list[[i]], mean = mu.t1,log=TRUE)
a2 <- dmvnorm(x=x.list[[i]], mean = mu.t2,log=TRUE)
a3 <- dmvnorm(x=x.list[[i]], mean = mu.t3,log=TRUE)
total <- exp(a1)+exp(a2)+exp(a3)
if (i %in% idx1) true.p[i] <- exp(a1)/total
else if (i %in% idx2) true.p[i] <- exp(a2)/total
else if (i %in% idx3) true.p[i] <- exp(a3)/total
else warning(paste0("No class was assigned for obs ",i))
}
sum(true.p <0.9)
set.seed(seed)
K <- 3
# Check the Division part
if (n %% K != 0) message(n, " is not divisible by ",K)
##== Class labeling at y ==##
y <- rep(1,n)
quotient <- n %/% K
a <- c(1:n)
num <- seq(0, n, quotient)
label <- c(1:K)
for (i in 1:K){
y[(num[i]+1):num[i+1]] <- label[i]
}
##== Labeling End ==##
beta1 <- seq(0, (K-1), 1) * beta
# spatio temporal predictor
n.t <- length(t)
# covariance matrix of gaussian process
if (cov=="I") {
Sigma <- error*diag(n.t)
} else if(cov=="AR") {
rho <- rho # 0.7
idx.t <- c(1:n.t)
Sigma <- error * outer(idx.t, idx.t, function(a, b) rho^abs(a-b))
} else if(cov=="CS") {
rho <- rho # 0.3
tmp <- matrix(rho,n.t,n.t)
diag(tmp) <- 1
Sigma <- error * tmp
} else warning("covariance structure was not specified")
# empty x.list
x.list <- as.list(1:n)
# Divide index
idx1 <- which(y==1)
idx2 <- which(y==2)
idx3 <- which(y==3)
# Create mean vector
mu.t1 <- sin(t)+ beta1[1]
mu.t2 <- sin(t)+ beta1[2]
mu.t3 <- sin(t)+ beta1[3]
# x.list
x.list <- as.list(1:n)
for (i in 1:n) {
if(i %in% idx1) x.list[[i]] <- rmvnorm(1, mu.t1, Sigma)
else if(i %in% idx2) x.list[[i]] <- rmvnorm(1, mu.t2, Sigma)
else if(i %in% idx3) x.list[[i]] <- rmvnorm(1, mu.t3, Sigma)
}
# Calculate the True p
true.p <- rep(0,n)
for(i in 1:n){
a1 <- dmvnorm(x=x.list[[i]], mean = mu.t1,log=TRUE)
a2 <- dmvnorm(x=x.list[[i]], mean = mu.t2,log=TRUE)
a3 <- dmvnorm(x=x.list[[i]], mean = mu.t3,log=TRUE)
total <- exp(a1)+exp(a2)+exp(a3)
if (i %in% idx1) true.p[i] <- exp(a1)/total
else if (i %in% idx2) true.p[i] <- exp(a2)/total
else if (i %in% idx3) true.p[i] <- exp(a3)/total
else warning(paste0("No class was assigned for obs ",i))
}
sum(true.p <0.9)
set.seed(seed)
K <- 3
# Check the Division part
if (n %% K != 0) message(n, " is not divisible by ",K)
##== Class labeling at y ==##
y <- rep(1,n)
quotient <- n %/% K
a <- c(1:n)
num <- seq(0, n, quotient)
label <- c(1:K)
for (i in 1:K){
y[(num[i]+1):num[i+1]] <- label[i]
}
##== Labeling End ==##
beta1 <- seq(0, (K-1), 1) * beta
# spatio temporal predictor
n.t <- length(t)
# covariance matrix of gaussian process
if (cov=="I") {
Sigma <- error*diag(n.t)
} else if(cov=="AR") {
rho <- rho # 0.7
idx.t <- c(1:n.t)
Sigma <- error * outer(idx.t, idx.t, function(a, b) rho^abs(a-b))
} else if(cov=="CS") {
rho <- rho # 0.3
tmp <- matrix(rho,n.t,n.t)
diag(tmp) <- 1
Sigma <- error * tmp
} else warning("covariance structure was not specified")
# empty x.list
x.list <- as.list(1:n)
# Divide index
idx1 <- which(y==1)
idx2 <- which(y==2)
idx3 <- which(y==3)
# Create mean vector
mu.t1 <- t + beta1[1]
mu.t2 <- t + beta1[2]
mu.t3 <- t + beta1[3]
# x.list
x.list <- as.list(1:n)
for (i in 1:n) {
if(i %in% idx1) x.list[[i]] <- rmvnorm(1, mu.t1, Sigma)
else if(i %in% idx2) x.list[[i]] <- rmvnorm(1, mu.t2, Sigma)
else if(i %in% idx3) x.list[[i]] <- rmvnorm(1, mu.t3, Sigma)
}
# Calculate the True p
true.p <- rep(0,n)
for(i in 1:n){
a1 <- dmvnorm(x=x.list[[i]], mean = mu.t1,log=TRUE)
a2 <- dmvnorm(x=x.list[[i]], mean = mu.t2,log=TRUE)
a3 <- dmvnorm(x=x.list[[i]], mean = mu.t3,log=TRUE)
total <- exp(a1)+exp(a2)+exp(a3)
if (i %in% idx1) true.p[i] <- exp(a1)/total
else if (i %in% idx2) true.p[i] <- exp(a2)/total
else if (i %in% idx3) true.p[i] <- exp(a3)/total
else warning(paste0("No class was assigned for obs ",i))
}
sum(true.p <0.9)
